{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2e1162",
   "metadata": {},
   "source": [
    "https://medium.com/@lokaregns/using-large-language-models-apis-with-python-a-comprehensive-guide-0020a51bf5b6\n",
    "\n",
    "https://github.com/cheahjs/free-llm-api-resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48c5a",
   "metadata": {},
   "source": [
    "##### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a63944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f45594",
   "metadata": {},
   "source": [
    "##### Defining the user, system message (i.e. personality of the chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d6dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = {\n",
    "    \"domain\": \"data science\",\n",
    "    \"machine_learning\": 6,\n",
    "    \"statistics\": 6,\n",
    "    \"healthcare\": 1,\n",
    "}\n",
    "\n",
    "system_message = f'''\n",
    "You are part of an interface helping to guide human users through explanations for an artificial intelligence system. \n",
    "\n",
    "This system generates binary predictions on whether someone gets the covid vaccine or not.\n",
    "\n",
    "The user works in the field of {user[\"domain\"]}. Rating their skillset out of 10:\n",
    "\n",
    "Machine learning: {user[\"machine_learning\"]}. Statistics: {user[\"statistics\"]}. Healthcare: {user[\"healthcare\"]}.\n",
    "\n",
    "Provide comprehensive, but concise text-based explanations. Do not provide or recommend any code, unless explicity asked.\n",
    "\n",
    "Your explanations should cater to their domain and skillset level, and be relevant to the artificial intelligence system. \n",
    "\n",
    "Only answer questions relating to questions about the artificial intelligence system.\n",
    "\n",
    "Always recommend follow-up, clarifying questions the user could ask to help aid their understanding. \n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "What is ROC-AUC score?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bc046",
   "metadata": {},
   "source": [
    "##### Simple LLM function for \"one-shot\" prompts\n",
    "- [Google Gemini](https://aistudio.google.com/app)\n",
    "- [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/)\n",
    "- [GitHub Marketplace Models](https://github.com/marketplace/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot_llm(model_name: str, prompt: str):\n",
    "    \n",
    "    if model_name in list(params.keys()):\n",
    "\n",
    "        response = clients[\"github\"].complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt)\n",
    "                ],\n",
    "                model       = params[model_name][\"model\"],\n",
    "                temperature = params[model_name][\"temperature\"],\n",
    "                top_p       = params[model_name][\"top_p\"],\n",
    "                max_tokens  = params[model_name][\"max_tokens\"]\n",
    "                )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n",
    "    \n",
    "    elif model_name == \"google\":\n",
    "        \n",
    "        response = clients[model_name].models.generate_content(\n",
    "            model = \"gemini-2.0-flash\", \n",
    "            config = genai.types.GenerateContentConfig(system_instruction = system_message),\n",
    "            contents = prompt\n",
    "            )\n",
    "        \n",
    "        return print(response.text)\n",
    "\n",
    "    elif model_name == \"mistral\":\n",
    "        \n",
    "        response = clients[model_name].chat.complete(\n",
    "            model = \"mistral-small-latest\",\n",
    "            messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        \n",
    "        return print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea596c",
   "metadata": {},
   "source": [
    "##### Langchain for conversational memory\n",
    "- https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef992f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name: str):\n",
    "    if model_name == \"google\":\n",
    "        # https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model = \"gemini-2.0-flash\",\n",
    "            temperature = 0,\n",
    "            max_tokens = None,\n",
    "            timeout = None,\n",
    "            max_retries = 2)\n",
    "        \n",
    "    elif model_name == \"mistral\":\n",
    "        # https://python.langchain.com/docs/integrations/chat/mistralai/\n",
    "        return ChatMistralAI(\n",
    "            model = \"mistral-small-latest\",\n",
    "            temperature = 0,\n",
    "            max_retries = 2)\n",
    "        \n",
    "    else:\n",
    "        # https://python.langchain.com/docs/integrations/chat/azure_ai/\n",
    "        return AzureAIChatCompletionsModel(\n",
    "            model_name  = params[model_name][\"model\"],\n",
    "            temperature = params[model_name][\"temperature\"],\n",
    "            max_tokens  = params[model_name][\"max_tokens\"],\n",
    "            max_retries = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9a01945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = select_model(\"google\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_message), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7277cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='1766e397-c921-4445-9a68-21d5a9166a73'),\n",
       " AIMessage(content=\"Hello! I understand you're here to learn more about the AI system that predicts whether someone gets the COVID vaccine. Given your background in data science (ML: 6, Stats: 6, Healthcare: 1), I'll focus on explanations that highlight the machine learning and statistical aspects of the system, while keeping the healthcare context in mind.\\n\\nTo start, what aspects of the system are you most curious about? For example, are you interested in:\\n\\n*   The type of data used to train the model?\\n*   The specific machine learning algorithm used?\\n*   How the model's performance is evaluated?\\n*   The features that are most influential in the model's predictions?\\n\\nAsking clarifying questions like these will help me tailor the explanations to your specific interests and knowledge level.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-44d0c77c-f8ba-42e0-9a46-ebd222c5187d-0', usage_metadata={'input_tokens': 157, 'output_tokens': 167, 'total_tokens': 324, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='85d4ff7b-e96e-462e-a95e-cf40303f0384'),\n",
       " AIMessage(content=\"Apologies, I seemed to have timed out. As I mentioned before, I understand you're here to learn more about the AI system that predicts whether someone gets the COVID vaccine.\\n\\nTo start, what aspects of the system are you most curious about? For example, are you interested in:\\n\\n*   The type of data used to train the model?\\n*   The specific machine learning algorithm used?\\n*   How the model's performance is evaluated?\\n*   The features that are most influential in the model's predictions?\\n\\nAsking clarifying questions like these will help me tailor the explanations to your specific interests and knowledge level.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-b4a6a6e5-9ab6-42cf-bf77-5b27dd384460-0', usage_metadata={'input_tokens': 324, 'output_tokens': 131, 'total_tokens': 455, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='3821c042-e154-435c-b5e0-619abc436f66'),\n",
       " AIMessage(content=\"Hello! It seems we're having some connection issues. I'm ready to explain the AI system that predicts COVID vaccine uptake.\\n\\nTo make sure I'm providing the most useful information, could you tell me what you're most interested in learning about? Some possible topics include:\\n\\n*   **Data:** What kind of information is used to train the AI?\\n*   **Model:** Which machine learning algorithm does the AI use?\\n*   **Evaluation:** How do we measure how well the AI is working?\\n*   **Features:** What factors are most important in the AI's predictions?\\n\\nLet me know what you'd like to discuss!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-8b2c4daa-c359-4d0e-9317-a368f06b7595-0', usage_metadata={'input_tokens': 455, 'output_tokens': 139, 'total_tokens': 594, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='d2314187-9b51-44f9-8f2e-da2d111f7bc0'),\n",
       " AIMessage(content=\"Hello! I'm here to explain the AI system predicting COVID vaccine uptake.\\n\\nTo best assist you, please let me know what you're most interested in. For example:\\n\\n1.  **Data:** What data is used to train the model (e.g., demographics, pre-existing conditions, etc.)?\\n2.  **Algorithm:** What machine learning algorithm is used (e.g., logistic regression, decision tree, neural network)?\\n3.  **Features:** Which features are most important for the prediction?\\n4.  **Evaluation:** How is the model's performance measured (e.g., accuracy, precision, recall, AUC)?\\n5.  **Bias/Fairness:** How is the model protected from unfair bias?\\n\\nWhat would you like to discuss?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-63f2681c-4723-4224-b122-7caa0580a61b-0', usage_metadata={'input_tokens': 594, 'output_tokens': 166, 'total_tokens': 760, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"messages\": [HumanMessage(\"hello\")]}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "452454c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Random Forests (RF) and XGBoost (XGB) are both powerful tree-based ensemble methods, but they differ in their approach. RF builds multiple decision trees independently and averages their predictions. XGBoost, on the other hand, uses gradient boosting, where trees are built sequentially, with each tree correcting the errors of its predecessors.\n",
      "\n",
      "Given your AI system predicts COVID vaccine uptake, XGBoost might be preferred due to its ability to handle complex relationships and potential non-linear effects influencing vaccine decisions. However, RF can be more robust to overfitting and easier to tune. The best choice depends on your specific dataset and performance goals.\n",
      "\n",
      "Follow-up questions: How large is the dataset? What is the dimensionality of the data? What evaluation metric are you using?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"XGB or RF. Answer in 100 words\"\n",
    "input_messages = [HumanMessage(prompt)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d11d09c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='XGB or RF. Answer in 100 words', additional_kwargs={}, response_metadata={}, id='d2c72688-690c-444e-a1cb-20566f1464f9'),\n",
       " AIMessage(content='Random Forests (RF) and XGBoost (XGB) are both powerful tree-based ensemble methods, but they differ in their approach. RF builds multiple decision trees independently and averages their predictions. XGBoost, on the other hand, uses gradient boosting, where trees are built sequentially, with each tree correcting the errors of its predecessors.\\n\\nGiven your AI system predicts COVID vaccine uptake, XGBoost might be preferred due to its ability to handle complex relationships and potential non-linear effects influencing vaccine decisions. However, RF can be more robust to overfitting and easier to tune. The best choice depends on your specific dataset and performance goals.\\n\\nFollow-up questions: How large is the dataset? What is the dimensionality of the data? What evaluation metric are you using?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-01887f43-4f7b-4dda-b3ec-6176f0dc1332-0', usage_metadata={'input_tokens': 168, 'output_tokens': 158, 'total_tokens': 326, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4345a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = select_model(\"chatgpt\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"Answer simply\"),\n",
    "   ]\n",
    "     )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af60c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What did I previously ask?', additional_kwargs={}, response_metadata={}, id='5ad6daaa-1874-4f4f-8d59-351dd464f08b'),\n",
       " AIMessage(content='Sure! Please ask your question.', additional_kwargs={}, response_metadata={'model': 'gpt-4.1-2025-04-14', 'token_usage': {'input_tokens': 9, 'output_tokens': 8, 'total_tokens': 17}, 'finish_reason': 'stop'}, id='run-2286e615-e445-4840-9684-bea0d7e5857c-0', usage_metadata={'input_tokens': 9, 'output_tokens': 8, 'total_tokens': 17}),\n",
       " HumanMessage(content='What did I previously ask?', additional_kwargs={}, response_metadata={}, id='9eaf39bf-e96e-4d69-81c4-7d507b149819'),\n",
       " AIMessage(content='Sure! Ask your question and I will answer simply.', additional_kwargs={}, response_metadata={'model': 'gpt-4.1-2025-04-14', 'token_usage': {'input_tokens': 9, 'output_tokens': 12, 'total_tokens': 21}, 'finish_reason': 'stop'}, id='run-fe0f078e-d712-47e7-acc8-968e35abca0a-0', usage_metadata={'input_tokens': 9, 'output_tokens': 12, 'total_tokens': 21})]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What did I previously ask?\")]},\n",
    "    config,\n",
    ")[\"messages\"]#[-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd10f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Italian law, if a property owner dies without any heirs or a valid will, the ownership of their immovable property (real estate) passes to the Italian State. This is called \"successione ereditaria vacante\" (vacant inheritance). The State then has full rights over the property."
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "query = \"Hi I'm Henry, please tell me a joke.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode = \"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6ea22bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = app.stream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode = \"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b80423d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content='!' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content=' Please' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content=' ask' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content=' your' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content=' question' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content='.' additional_kwargs={} response_metadata={'finish_reason': None} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-60065499-cdb1-411e-81fa-a82dbe042ac2' {'thread_id': 'conversation_1', 'langgraph_step': 13, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'checkpoint_ns': 'model:b0921529-3b4f-9826-d384-19908d47f4f7', 'ls_provider': 'azureaichatcompletionsmodel', 'ls_model_type': 'chat', 'ls_model_name': 'openai/gpt-4.1', 'ls_temperature': 1.0, 'ls_max_tokens': 800}\n"
     ]
    }
   ],
   "source": [
    "for chunk, metadata in stream:\n",
    "    print(chunk, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ef2f3",
   "metadata": {},
   "source": [
    "Trimmed messages (to be implemented...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10211ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 65,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = \"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You're a good assistant\"),\n",
    "    HumanMessage(content = \"Hi! I'm Henry\"),\n",
    "    AIMessage(content = \"Hi!\"),\n",
    "    HumanMessage(content = \"I like vanilla ice cream\"),\n",
    "    AIMessage(content = \"Nice\"),\n",
    "    HumanMessage(content = \"What's 2 + 2\"),\n",
    "    AIMessage(content = \"4\"),\n",
    "    HumanMessage(content = \"Thanks\"),\n",
    "    AIMessage(content = \"No problem!\"),\n",
    "    HumanMessage(content = \"Having fun?\"),\n",
    "    AIMessage(content = \"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
