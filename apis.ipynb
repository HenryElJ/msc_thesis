{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2e1162",
   "metadata": {},
   "source": [
    "https://medium.com/@lokaregns/using-large-language-models-apis-with-python-a-comprehensive-guide-0020a51bf5b6\n",
    "\n",
    "https://github.com/cheahjs/free-llm-api-resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48c5a",
   "metadata": {},
   "source": [
    "##### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a63944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f45594",
   "metadata": {},
   "source": [
    "##### Defining the user, system message (i.e. personality of the chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bc046",
   "metadata": {},
   "source": [
    "##### Simple LLM function for \"one-shot\" prompts\n",
    "- [Google Gemini](https://aistudio.google.com/app)\n",
    "- [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/)\n",
    "- [GitHub Marketplace Models](https://github.com/marketplace/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot_llm(model_name: str, prompt: str):\n",
    "    \n",
    "    if model_name in list(params.keys()):\n",
    "\n",
    "        response = clients[\"github\"].complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt)\n",
    "                ],\n",
    "                model       = params[model_name][\"model\"],\n",
    "                temperature = params[model_name][\"temperature\"],\n",
    "                top_p       = params[model_name][\"top_p\"],\n",
    "                max_tokens  = params[model_name][\"max_tokens\"]\n",
    "                )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n",
    "    \n",
    "    elif model_name == \"google\":\n",
    "        \n",
    "        response = clients[model_name].models.generate_content(\n",
    "            model = \"gemini-2.0-flash\", \n",
    "            config = genai.types.GenerateContentConfig(system_instruction = system_message),\n",
    "            contents = prompt\n",
    "            )\n",
    "        \n",
    "        return print(response.text)\n",
    "\n",
    "    elif model_name == \"mistral\":\n",
    "        \n",
    "        response = clients[model_name].chat.complete(\n",
    "            model = \"mistral-small-latest\",\n",
    "            messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        \n",
    "        return print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea596c",
   "metadata": {},
   "source": [
    "##### Langchain for conversational memory\n",
    "- https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef992f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name: str):\n",
    "    if model_name == \"google\":\n",
    "        # https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model = \"gemini-2.0-flash\",\n",
    "            temperature = 0,\n",
    "            max_tokens = None,\n",
    "            timeout = None,\n",
    "            max_retries = 2)\n",
    "        \n",
    "    elif model_name == \"mistral\":\n",
    "        # https://python.langchain.com/docs/integrations/chat/mistralai/\n",
    "        return ChatMistralAI(\n",
    "            model = \"mistral-small-latest\",\n",
    "            temperature = 0,\n",
    "            max_retries = 2)\n",
    "        \n",
    "    else:\n",
    "        # https://python.langchain.com/docs/integrations/chat/azure_ai/\n",
    "        return AzureAIChatCompletionsModel(\n",
    "            model_name  = params[model_name][\"model\"],\n",
    "            temperature = params[model_name][\"temperature\"],\n",
    "            max_tokens  = params[model_name][\"max_tokens\"],\n",
    "            max_retries = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a9a01945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = select_model(\"google\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Answer in 30 words\"), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "def stream_output(stream):\n",
    "    for chunk, _ in stream:\n",
    "        print(chunk.content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3646e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is a cat?', additional_kwargs={}, response_metadata={}, id='08204268-02f0-41be-81b4-57e8bbd2e71c'),\n",
       " AIMessage(content='A small, domesticated carnivorous mammal with soft fur.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-68ad8514-47d7-4d74-ba14-5b6e5bdf0d51-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = [HumanMessage(\"What is a cat?\")]\n",
    "# app.invoke({\"messages\": query}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a2c60323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While cats don't form friendships like humans, they can develop strong bonds with other cats, showing affection through grooming, playing, and sharing resources."
     ]
    }
   ],
   "source": [
    "query = [HumanMessage(\"Do cats have friends?\")]\n",
    "# app.stream({\"messages\": query}, config, stream_mode = \"messages\")\n",
    "\n",
    "stream_output(\n",
    "    app.stream({\"messages\": query}, config, stream_mode = \"messages\")\n",
    "    ) # Running this function makes all the difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f4aa505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = [HumanMessage(\"Do _you_ have friends?\")]\n",
    "app.invoke({\"messages\": query}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "82388244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config)[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ef4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pregel.stream() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk)\n",
      "\u001b[0;31mTypeError\u001b[0m: Pregel.stream() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "for chunk in app.stream(\"Hello\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bd10f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Italian law, if a property owner dies without any heirs or a valid will, the ownership of their immovable property (real estate) passes to the Italian State. This is called \"successione ereditaria vacante\" (vacant inheritance). The State then has full rights over the property."
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "query = \"Hi I'm Henry, please tell me a joke.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode = \"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6ea22bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = app.stream(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    "    stream_mode = \"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ef2f3",
   "metadata": {},
   "source": [
    "Trimmed messages (to be implemented...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e10211ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 65,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = \"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You're a good assistant\"),\n",
    "    HumanMessage(content = \"Hi! I'm Henry\"),\n",
    "    AIMessage(content = \"Hi!\"),\n",
    "    HumanMessage(content = \"I like vanilla ice cream\"),\n",
    "    AIMessage(content = \"Nice\"),\n",
    "    HumanMessage(content = \"What's 2 + 2\"),\n",
    "    AIMessage(content = \"4\"),\n",
    "    HumanMessage(content = \"Thanks\"),\n",
    "    AIMessage(content = \"No problem!\"),\n",
    "    HumanMessage(content = \"Having fun?\"),\n",
    "    AIMessage(content = \"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
