{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2e1162",
   "metadata": {},
   "source": [
    "https://medium.com/@lokaregns/using-large-language-models-apis-with-python-a-comprehensive-guide-0020a51bf5b6\n",
    "\n",
    "https://github.com/cheahjs/free-llm-api-resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48c5a",
   "metadata": {},
   "source": [
    "##### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a63944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bc046",
   "metadata": {},
   "source": [
    "##### Simple LLM function for \"one-shot\" prompts\n",
    "- [Google Gemini](https://aistudio.google.com/app)\n",
    "- [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/)\n",
    "- [GitHub Marketplace Models](https://github.com/marketplace/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot_llm(model_name: str, prompt: str, system_message: str = \"Answer briefly\"):\n",
    "    \n",
    "    if model_name == \"google\":\n",
    "        \n",
    "        response = clients[model_name].models.generate_content(\n",
    "            model = \"gemini-2.0-flash\", \n",
    "            config = genai.types.GenerateContentConfig(system_instruction = system_message),\n",
    "            contents = prompt\n",
    "            )\n",
    "        \n",
    "        return print(response.text)\n",
    "\n",
    "    elif model_name == \"mistral\":\n",
    "        \n",
    "        response = clients[model_name].chat.complete(\n",
    "            model = \"mistral-small-latest\",\n",
    "            messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n",
    "\n",
    "    elif model_name == \"mistral-large\":\n",
    "\n",
    "        client = Mistral(\n",
    "            api_key = api_keys[\"github\"],\n",
    "            server_url = \"https://models.inference.ai.azure.com\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt),\n",
    "            ],\n",
    "            model       = params[model_name][\"model\"],\n",
    "            temperature = params[model_name][\"temperature\"],\n",
    "            top_p       = params[model_name][\"top_p\"],\n",
    "            max_tokens  = params[model_name][\"max_tokens\"]\n",
    "        )\n",
    "\n",
    "        return print(response.choices[0].message.content)\n",
    "    \n",
    "    elif model_name in list(params.keys()):\n",
    "\n",
    "        response = clients[\"github\"].complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt)\n",
    "                ],\n",
    "                model       = params[model_name][\"model\"],\n",
    "                temperature = params[model_name][\"temperature\"],\n",
    "                top_p       = params[model_name][\"top_p\"],\n",
    "                max_tokens  = params[model_name][\"max_tokens\"]\n",
    "                )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f033b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Large Language Model trained by Mistral AI.\n"
     ]
    }
   ],
   "source": [
    "oneshot_llm(\"mistral-large\", \"what model are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea596c",
   "metadata": {},
   "source": [
    "##### Langchain for conversational memory\n",
    "- https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9a01945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = select_model(\"google\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Answer in 30 words\"), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "def stream_output(stream):\n",
    "    for chunk, _ in stream:\n",
    "        print(chunk.content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3646e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is a cat?', additional_kwargs={}, response_metadata={}, id='08204268-02f0-41be-81b4-57e8bbd2e71c'),\n",
       " AIMessage(content='A small, domesticated carnivorous mammal with soft fur.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-68ad8514-47d7-4d74-ba14-5b6e5bdf0d51-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app.invoke({\"messages\": [HumanMessage(\"What is a cat?\")]}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While cats don't form friendships like humans, they can develop strong bonds with other cats, showing affection through grooming, playing, and sharing resources."
     ]
    }
   ],
   "source": [
    "stream_output(\n",
    "    app.stream({\"messages\": [HumanMessage(\"Do cats have friends?\")]}, config, stream_mode = \"messages\")\n",
    "    ) # Running this `stream_output(...)` function makes all the difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4aa505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"messages\": [HumanMessage(\"Do _you_ have friends?\")]}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "82388244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config)[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d361b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running invoke -> stream -> invoke, or stream -> stream -> ... doesnt commit streamed response to the app's states\n",
    "# Need to use the stream_output function (something is happening in the backend where Langchain identifies we want to use streaming, and also commits messages to the app's state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e5299",
   "metadata": {},
   "source": [
    "##### Langchain with multimodal input\n",
    "- https://python.langchain.com/docs/how_to/multimodal_inputs/\n",
    "\n",
    "- Note: ChatMistralAI does not have multimodal input (e.g. images). \n",
    "    - See more [here.](https://python.langchain.com/docs/integrations/chat/)\n",
    "\n",
    "- Gemini allows URL images\n",
    "- The rest require the image data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6b32a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch image data\n",
    "model = select_model(\"google\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Answer in 250 words or less. Explain to user in simple terms.\"), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "image_url = \"https://scikit-learn.org/1.1/_images/sphx_glr_plot_roc_002.png\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f62be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This plot is called a Receiver Operating Characteristic (ROC) curve, used to evaluate the performance of a classification model, especially when dealing with multiple classes.\\n\\n*   **Axes:** The x-axis represents the False Positive Rate (incorrectly predicted positives), and the y-axis represents the True Positive Rate (correctly predicted positives).\\n\\n*   **Curves:** Each colored line represents the ROC curve for a specific class (0, 1, and 2). The closer the curve is to the top-left corner, the better the model is at distinguishing that class.\\n\\n*   **Averages:** The plot also shows two average ROC curves:\\n    *   **Micro-average:** Calculates the average performance across all classes.\\n    *   **Macro-average:** Calculates the average performance for each class individually and then averages those scores.\\n\\n*   **Area Under the Curve (AUC):** The number in the legend indicates the AUC for each curve. A higher AUC (closer to 1) means better performance.\\n\\n*   **Dashed Line:** The dashed line represents a random classifier. A good model will have curves above this line.'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = HumanMessage([\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Explain to me what this plot shows\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source_type\": \"base64\",\n",
    "            \"data\": image_data,\n",
    "            \"mime_type\": \"image/jpg\",\n",
    "        }\n",
    "        # {\n",
    "        #     \"type\": \"image\",\n",
    "        #     \"source_type\": \"url\",\n",
    "        #     \"url\": image_url,\n",
    "        # }\n",
    "        ])\n",
    "\n",
    "response = app.invoke({\"messages\": [prompt]}, config)\n",
    "app.get_state(config)[0][\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ef2f3",
   "metadata": {},
   "source": [
    "##### Langchain trimmed messages (to be implemented...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e10211ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 65,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = \"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You're a good assistant\"),\n",
    "    HumanMessage(content = \"Hi! I'm Henry\"),\n",
    "    AIMessage(content = \"Hi!\"),\n",
    "    HumanMessage(content = \"I like vanilla ice cream\"),\n",
    "    AIMessage(content = \"Nice\"),\n",
    "    HumanMessage(content = \"What's 2 + 2\"),\n",
    "    AIMessage(content = \"4\"),\n",
    "    HumanMessage(content = \"Thanks\"),\n",
    "    AIMessage(content = \"No problem!\"),\n",
    "    HumanMessage(content = \"Having fun?\"),\n",
    "    AIMessage(content = \"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
