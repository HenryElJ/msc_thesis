{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2e1162",
   "metadata": {},
   "source": [
    "https://medium.com/@lokaregns/using-large-language-models-apis-with-python-a-comprehensive-guide-0020a51bf5b6\n",
    "\n",
    "https://github.com/cheahjs/free-llm-api-resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48c5a",
   "metadata": {},
   "source": [
    "##### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a63944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bc046",
   "metadata": {},
   "source": [
    "##### Simple LLM function for \"one-shot\" prompts\n",
    "- [Google Gemini](https://aistudio.google.com/app)\n",
    "- [Mistral AI](https://docs.mistral.ai/getting-started/quickstart/)\n",
    "- [GitHub Marketplace Models](https://github.com/marketplace/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot_llm(model_name: str, prompt: str, system_message: str = \"Answer briefly\"):\n",
    "    \n",
    "    if model_name == \"google\":\n",
    "        \n",
    "        response = clients[model_name].models.generate_content(\n",
    "            model = \"gemini-2.0-flash\", \n",
    "            config = genai.types.GenerateContentConfig(system_instruction = system_message),\n",
    "            contents = prompt\n",
    "            )\n",
    "        \n",
    "        return print(response.text)\n",
    "\n",
    "    elif model_name == \"mistral\":\n",
    "        \n",
    "        response = clients[model_name].chat.complete(\n",
    "            model = \"mistral-small-latest\",\n",
    "            messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n",
    "\n",
    "    elif model_name == \"mistral-large\":\n",
    "\n",
    "        client = Mistral(\n",
    "            api_key = api_keys[\"github\"],\n",
    "            server_url = \"https://models.inference.ai.azure.com\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt),\n",
    "            ],\n",
    "            model       = params[model_name][\"model\"],\n",
    "            temperature = params[model_name][\"temperature\"],\n",
    "            top_p       = params[model_name][\"top_p\"],\n",
    "            max_tokens  = params[model_name][\"max_tokens\"]\n",
    "        )\n",
    "\n",
    "        return print(response.choices[0].message.content)\n",
    "    \n",
    "    elif model_name in list(params.keys()):\n",
    "\n",
    "        response = clients[\"github\"].complete(\n",
    "            messages = [\n",
    "                SystemMessage(system_message),\n",
    "                UserMessage(prompt)\n",
    "                ],\n",
    "                model       = params[model_name][\"model\"],\n",
    "                temperature = params[model_name][\"temperature\"],\n",
    "                top_p       = params[model_name][\"top_p\"],\n",
    "                max_tokens  = params[model_name][\"max_tokens\"]\n",
    "                )\n",
    "        \n",
    "        return print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f033b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Large Language Model trained by Mistral AI.\n"
     ]
    }
   ],
   "source": [
    "oneshot_llm(\"mistral-large\", \"what model are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea596c",
   "metadata": {},
   "source": [
    "##### Langchain for conversational memory\n",
    "- https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9a01945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = select_model(\"google\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Answer in 30 words\"), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "def stream_output(stream):\n",
    "    for chunk, _ in stream:\n",
    "        print(chunk.content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3646e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is a cat?', additional_kwargs={}, response_metadata={}, id='08204268-02f0-41be-81b4-57e8bbd2e71c'),\n",
       " AIMessage(content='A small, domesticated carnivorous mammal with soft fur.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-68ad8514-47d7-4d74-ba14-5b6e5bdf0d51-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app.invoke({\"messages\": [HumanMessage(\"What is a cat?\")]}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While cats don't form friendships like humans, they can develop strong bonds with other cats, showing affection through grooming, playing, and sharing resources."
     ]
    }
   ],
   "source": [
    "stream_output(\n",
    "    app.stream({\"messages\": [HumanMessage(\"Do cats have friends?\")]}, config, stream_mode = \"messages\")\n",
    "    ) # Running this `stream_output(...)` function makes all the difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4aa505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"messages\": [HumanMessage(\"Do _you_ have friends?\")]}, config,)[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "82388244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do cats have friends?', additional_kwargs={}, response_metadata={}, id='57c79e80-58ab-44f5-8bcc-31fb551199a5'),\n",
       " AIMessage(content='Yes, cats can form friendships with other cats.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run-fc192720-85c9-4809-b5fd-1befaff0c417', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='Do _you_ have friends?', additional_kwargs={}, response_metadata={}, id='37aa99f9-dc0c-4e20-90f4-fb79c8dc504f'),\n",
       " AIMessage(content=\"As an AI, I don't have friends.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-f646ac99-9fc6-4c9a-9938-164ffb1fbbba-0', usage_metadata={'input_tokens': 30, 'output_tokens': 12, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config)[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d361b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running invoke -> stream -> invoke, or stream -> stream -> ... doesnt commit streamed response to the app's states\n",
    "# Need to use the stream_output function (something is happening in the backend where Langchain identifies we want to use streaming, and also commits messages to the app's state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e5299",
   "metadata": {},
   "source": [
    "##### Langchain with multimodal input\n",
    "- https://python.langchain.com/docs/how_to/multimodal_inputs/\n",
    "\n",
    "- Note: ChatMistralAI does not have multimodal input (e.g. images). \n",
    "    - See more [here.](https://python.langchain.com/docs/integrations/chat/)\n",
    "\n",
    "- Gemini allows URL images\n",
    "- The rest require the image data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6b32a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch image data\n",
    "import base64, httpx\n",
    "\n",
    "model = select_model(\"chatgpt\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Answer in 250 words or less. Explain to user in simple terms.\"), \n",
    "     MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ]\n",
    "     )\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema = MessagesState)\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "image_url = \"https://scikit-learn.org/1.1/_images/sphx_glr_plot_roc_002.png\"\n",
    "image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b58f62be",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(invalid_value) Invalid value: 'image'. Supported values are: 'text', 'image_url', 'input_audio', 'refusal', 'audio', and 'file'.\nCode: invalid_value\nMessage: Invalid value: 'image'. Supported values are: 'text', 'image_url', 'input_audio', 'refusal', 'audio', and 'file'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m HumanMessage([\n\u001b[1;32m      2\u001b[0m         {\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         }\n\u001b[1;32m     16\u001b[0m         ])\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [prompt]}, config)\n\u001b[1;32m     19\u001b[0m app\u001b[38;5;241m.\u001b[39mget_state(config)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2795\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2794\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2797\u001b[0m     config,\n\u001b[1;32m   2798\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   2799\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2800\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   2801\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   2802\u001b[0m     checkpoint_during\u001b[38;5;241m=\u001b[39mcheckpoint_during,\n\u001b[1;32m   2803\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2805\u001b[0m ):\n\u001b[1;32m   2806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2807\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2433\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2428\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   2429\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2431\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   2432\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2433\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2434\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2435\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2436\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2437\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2438\u001b[0m         ):\n\u001b[1;32m   2439\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2440\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[177], line 14\u001b[0m, in \u001b[0;36mcall_model\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_model\u001b[39m(state: MessagesState):\n\u001b[1;32m     13\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke(prompt)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response]}\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:369\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    366\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    370\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    371\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    372\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    373\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    374\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    375\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    376\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    377\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    378\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    379\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:946\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    944\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    945\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:765\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 765\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    766\u001b[0m                 m,\n\u001b[1;32m    767\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    768\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    769\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    770\u001b[0m             )\n\u001b[1;32m    771\u001b[0m         )\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1011\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1011\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1012\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1015\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/langchain_azure_ai/chat_models/inference.py:565\u001b[0m, in \u001b[0;36mAzureAIChatCompletionsModel._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    559\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    563\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    564\u001b[0m     inference_messages \u001b[38;5;241m=\u001b[39m to_inference_message(messages)\n\u001b[0;32m--> 565\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcomplete(\n\u001b[1;32m    566\u001b[0m         messages\u001b[38;5;241m=\u001b[39minference_messages,\n\u001b[1;32m    567\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop,\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identifying_params,\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thesis_3.11/lib/python3.11/site-packages/azure/ai/inference/_patch.py:738\u001b[0m, in \u001b[0;36mChatCompletionsClient.complete\u001b[0;34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _models\u001b[38;5;241m.\u001b[39mStreamingChatCompletions(response)\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (invalid_value) Invalid value: 'image'. Supported values are: 'text', 'image_url', 'input_audio', 'refusal', 'audio', and 'file'.\nCode: invalid_value\nMessage: Invalid value: 'image'. Supported values are: 'text', 'image_url', 'input_audio', 'refusal', 'audio', and 'file'."
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage([\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Explain to me what this plot shows\",\n",
    "        },\n",
    "        # {\n",
    "        #     \"type\": \"image_url\",\n",
    "        #     \"image_url\": {\"url\": f\"data:image/jpg;base64,{image_data}\"},\n",
    "        #     \"mime_type\": \"image/jpg\",\n",
    "        # }\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source_type\": \"url\",\n",
    "            \"url\": image_url,\n",
    "        }\n",
    "        ])\n",
    "\n",
    "response = app.invoke({\"messages\": [prompt]}, config)\n",
    "app.get_state(config)[0][\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ef2f3",
   "metadata": {},
   "source": [
    "##### Langchain trimmed messages (to be implemented...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e10211ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 65,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = \"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You're a good assistant\"),\n",
    "    HumanMessage(content = \"Hi! I'm Henry\"),\n",
    "    AIMessage(content = \"Hi!\"),\n",
    "    HumanMessage(content = \"I like vanilla ice cream\"),\n",
    "    AIMessage(content = \"Nice\"),\n",
    "    HumanMessage(content = \"What's 2 + 2\"),\n",
    "    AIMessage(content = \"4\"),\n",
    "    HumanMessage(content = \"Thanks\"),\n",
    "    AIMessage(content = \"No problem!\"),\n",
    "    HumanMessage(content = \"Having fun?\"),\n",
    "    AIMessage(content = \"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema = State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer = memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
